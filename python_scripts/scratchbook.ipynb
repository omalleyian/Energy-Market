{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# Libs\n",
    "#-------------------------\n",
    " \n",
    "# External libs\n",
    "# %matplotlib qt         Fuck this line right here.\n",
    "import pymysql.cursors\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.neural_network import MLPRegressor as mlp\n",
    "sys.path.append(\"/home/omalleyian/Documents/energy_market_project/scripts\")\n",
    "from ercot_data_interface import ercot_data_interface\n",
    "from ARIMA import ARIMA\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# SQL environments\n",
    "#-------------------------\n",
    " \n",
    "HOST = \"localhost\"\n",
    "USER = \"root\" \n",
    "PASSWORD = \"Is79t5Is79t5\"\n",
    "DB = \"ercot_data\"\n",
    "\n",
    "#-------------------------\n",
    "# Functions\n",
    "#-------------------------\n",
    "\"\"\"\n",
    "Make a connection to MySQL\n",
    "Execute the MySQL query and return the resutls\n",
    "\"\"\"\n",
    "\n",
    "def execute_dict_query(query):\n",
    "    connection = pymysql.connect(host=HOST, \n",
    "                                 user=USER, \n",
    "                                 password=PASSWORD, \n",
    "                                 db=DB, \n",
    "                                 port=3306,\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "    \n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            # Create a new record\n",
    "            cursor.execute(query)\n",
    "            result = cursor.fetchall()\n",
    "            return result\n",
    "    finally:\n",
    "        connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(actual, forcast):\n",
    "    x = (np.asarray(actual)).flatten()\n",
    "    y = (np.asarray(forcast)).flatten()\n",
    "    return np.mean(np.square(np.abs((x - y)) / x))\n",
    "\n",
    "def mae(actual, forcast):\n",
    "    x = (np.asarray(actual)).flatten()\n",
    "    y = (np.asarray(forcast)).flatten()\n",
    "    return np.mean(np.abs((x - y)))\n",
    "\n",
    "def mase(actual, forcast):\n",
    "    x = (np.asarray(actual)).flatten()\n",
    "    y = (np.asarray(forcast)).flatten()\n",
    "    return np.mean(np.abs(x - y) / np.mean(np.abs(x[1:]-x[:-1])))\n",
    "    \n",
    "def hit_rate(actual, forcast):\n",
    "    x = (np.asarray(actual)).flatten()\n",
    "    y = (np.asarray(forcast)).flatten()\n",
    "    f_t = (x[1:] - x[:-1]) * (y[1:] - y[:-1])\n",
    "    # k is the subset of f_t where k[i] = f_t[i] iff f_t[i] > 0\n",
    "    k = [i for i in f_t if i > 0]\n",
    "    return np.mean(np.abs(k))\n",
    "\n",
    "def hit_rate2(actual, forcast):\n",
    "    x = (np.asarray(actual)).flatten()\n",
    "    y = (np.asarray(forcast)).flatten()\n",
    "    f_t = (x[1:] - x[:-1]) * (y[1:] - y[:-1])  \n",
    "    # k is the subset of y where k[i] = y[i] iff f_t[i] > 0\n",
    "    k = [y[i+1] for i in range(f_t.shape[0]) if f_t[i] > 0]\n",
    "    return np.mean(np.abs(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query data sorted by date and hour. SLOW!\n",
    "result_dict = execute_dict_query('select * from DAM_LMP0  \\\n",
    "                                where delivery_date < \"2016-12-31\" \\\n",
    "                                and delivery_date > \"2011-01-01\" \\\n",
    "                                order by delivery_date, hour_ending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract some data\n",
    "prices1 = [i['n0001VICTOR'] for i in result_dict]\n",
    "prices2 = [i['n0001'] for i in result_dict]\n",
    "hours = [i['hour_ending'].total_seconds() for i in result_dict]\n",
    "dates = [i['delivery_date'] for i in result_dict]\n",
    "plt.plot(prices1)\n",
    "plt.plot(prices2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percent Change\n",
    "k = 24\n",
    "p_t = np.array(prices1[k:])\n",
    "p_tk = np.array(prices1[:-k])\n",
    "v = np.log10(p_t) - np.log10(p_tk)\n",
    "plt.plot(v)\n",
    "# plt.show()\n",
    "\n",
    "# MAPE = mean( (actual - forcast) / actual)\n",
    "\n",
    "MAPE = np.mean(np.square(p_t-p_tk)/p_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic data plot\n",
    "price1 = [float(i['AMNCOWD_8']) for i in result_dict]\n",
    "price2 = [float(i['AZLES_K']) for i in result_dict]\n",
    "price3 = [float(i['n0001']) for i in result_dict]\n",
    "\n",
    "plt.plot(price1, label='1')\n",
    "plt.plot(price2, label='2')\n",
    "plt.plot(price3, label='2')\n",
    "plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(price1, 500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Autocorrelation\n",
    "plt.acorr(price1, maxlags=72)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(price1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extend this to get prices at different lags e.g. k-1, k-2, k-3\n",
    "max_lag = 168\n",
    "p_t = prices1[max_lag:]\n",
    "X = []\n",
    "for k in np.arange(0, max_lag, 24):\n",
    "    t_k = np.array(prices1[(max_lag-k):-k], dtype='float')\n",
    "    X.append(t_k)\n",
    "X = np.swapaxes(np.array(X[1:]), 0, 1)\n",
    "Y = [float(i) for i in p_t]\n",
    "# use sklearn.LinearRegression to fit (X, Y)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X,Y)\n",
    "\n",
    "print regr.predict(X)\n",
    "plt.plot(X)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Moving Average Model\n",
    "# Given a list X, time t, and m number of observations\n",
    "# Return the value of Y at time t of the previous m elements in X.\n",
    "\n",
    "def moving_average(X, t, m):\n",
    "    s = sum(X[(t-m):t])\n",
    "    avg = s/m\n",
    "    return avg\n",
    "\n",
    "print moving_average(prices1, 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ercot = ercot_data_interface(password=\"Is79t5Is79t5\")\n",
    "nodes_crr = ercot.get_CRR_nodes()\n",
    "nodes_all = ercot.all_nodes\n",
    "nodes_source = ercot.get_sources_sinks()\n",
    "df_2011 = ercot.query_prices(nodes_all[0], \"2011-01-01\",\"2011-12-31\")\n",
    "df_2012 = ercot.query_prices(nodes_all[0], \"2012-01-01\",\"2012-12-31\")\n",
    "matrix_2011 = df_2011.as_matrix()\n",
    "matrix_2012 = df_2012.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arima = ARIMA(p = 2, d = 0, q = 0, seasonal = 24)\n",
    "arima.fit(matrix_2011)\n",
    "arima.plot_predicted_vs_actual(matrix_2012)\n",
    "print arima.mae(matrix_2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arima Forcasting with variable horizon\n",
    "\n",
    "def arima_forcast(train, test, horizon, p, d, q, seasonal):\n",
    "    # First split data into subsets offset by 24, and fit ARIMA models to these subsets.\n",
    "    hours_train = []\n",
    "    hours_test = []\n",
    "    arima_models = []\n",
    "    loss = p + q + seasonal - 1\n",
    "\n",
    "    for i in range(horizon):\n",
    "        ind_train = np.arange(i,train.shape[0],horizon)\n",
    "        ind_test = np.arange(i,test.shape[0],horizon)\n",
    "\n",
    "        hours_train.append(train[ind_train])\n",
    "        hours_test.append(test[ind_test])\n",
    "\n",
    "        arima = ARIMA(p, d, q, seasonal)\n",
    "        arima.fit(train[ind_train])\n",
    "        arima_models.append(arima)\n",
    "\n",
    "    # Make predictions with the ARIMA models\n",
    "    # Merge the predictions into a single array to be ploted against actual data\n",
    "    merged_prediction = np.zeros(horizon * (test.shape[0] / horizon - loss))\n",
    "    merged_actual = np.zeros(horizon * (test.shape[0] / horizon - loss))\n",
    "    \n",
    "    for i in range(len(arima_models)):\n",
    "        prediction, actual = arima_models[i].predict(hours_test[i])\n",
    "        prediction = prediction.squeeze().tolist()\n",
    "        actual = actual.squeeze().tolist()\n",
    "\n",
    "        if len(prediction) < (len(test) / horizon) - loss:\n",
    "            prediction.extend(np.zeros((len(test) / horizon - loss) - len(prediction)))\n",
    "            actual.extend(np.zeros((len(test) / horizon - loss) - len(actual)))\n",
    "\n",
    "        for j in range(len(prediction)):\n",
    "            front = [0] * i\n",
    "            back = [0] * (horizon-1-i)\n",
    "            index = j * horizon\n",
    "\n",
    "            prediction[index:index] = front\n",
    "            actual[index:index] = front\n",
    "\n",
    "            prediction[index+i+1:index+i+1] = back\n",
    "            actual[index+i+1:index+i+1] = back\n",
    "\n",
    "        merged_prediction += prediction\n",
    "        merged_actual += actual\n",
    "        \n",
    "    return merged_prediction, merged_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_prediction, merged_actual = \\\n",
    "    arima_forcast(matrix_2011, matrix_2012, 24, 2, 0, 1, 1)\n",
    "plt.plot(merged_actual, label='actual')\n",
    "plt.plot(merged_prediction, label='prediction', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print \"Merged MAE: \" + str(mae(merged_actual, merged_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate mape for various models on the first 5 nodes\n",
    "for i in range(5):\n",
    "    func = hit_rate2\n",
    "    print \"MAE Calculation on Node\" + str(i) + \": \" + str(nodes_all[i])\n",
    "    # Load training and testing data from a node\n",
    "    df_train = ercot.query_prices(nodes_all[i], \"2011-01-01\",\"2015-05-23\")\n",
    "    df_test = ercot.query_prices(nodes_all[i], \"2015-05-23\",\"2016-05-23\")\n",
    "    matrix_train = df_train.as_matrix()\n",
    "    matrix_test = df_test.as_matrix()\n",
    "    \n",
    "    # Preform mean model calculations on data\n",
    "    m = np.mean(matrix_train)\n",
    "    m_matrix = [m] * len(matrix_test)\n",
    "    print func(matrix_test, m_matrix)\n",
    "    \n",
    "    # Perform random walk model calculations on data\n",
    "    walk = matrix_test[:-1]\n",
    "    walk_test = matrix_test[1:]\n",
    "    print func(walk_test, walk)\n",
    "    \n",
    "    # ARIMA(2,0,1,1)\n",
    "    arima = ARIMA(p = 2, d = 0, q = 1, seasonal = 1)\n",
    "    arima.fit(matrix_train)\n",
    "    actual, forcast = arima.predict(matrix_test)\n",
    "    print func(actual, forcast)\n",
    "    \n",
    "    # ARIMA(2,0,2,1)\n",
    "    arima = ARIMA(p = 2, d = 0, q = 2, seasonal = 1)\n",
    "    arima.fit(matrix_train)\n",
    "    actual, forcast = arima.predict(matrix_test)\n",
    "    print func(actual, forcast)\n",
    "    \n",
    "    # ARIMA(2,0,3,1)\n",
    "    arima = ARIMA(p = 2, d = 0, q = 3, seasonal = 1)\n",
    "    arima.fit(matrix_train)\n",
    "    actual, forcast = arima.predict(matrix_test)\n",
    "    print func(actual, forcast)\n",
    "    \n",
    "    # ARIMA(2,0,1,24)\n",
    "    arima = ARIMA(p = 2, d = 0, q = 1, seasonal = 24)\n",
    "    arima.fit(matrix_train)\n",
    "    actual, forcast = arima.predict(matrix_test)\n",
    "    print func(actual, forcast)    \n",
    "    \n",
    "    # ARIMA(2,0,2,24)\n",
    "    arima = ARIMA(p = 2, d = 0, q = 2, seasonal = 24)\n",
    "    arima.fit(matrix_train)\n",
    "    actual, forcast = arima.predict(matrix_test)\n",
    "    print func(actual, forcast)\n",
    "    \n",
    "    # ARIMA(2,0,3,24)\n",
    "    arima = ARIMA(p = 2, d = 0, q = 3, seasonal = 24)\n",
    "    arima.fit(matrix_train)\n",
    "    actual, forcast = arima.predict(matrix_test)\n",
    "    print func(actual, forcast)\n",
    "    \n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = ercot.query_prices(nodes_all[0], \"2011-01-01\",\"2015-05-23\")\n",
    "df_test = ercot.query_prices(nodes_all[0], \"2015-05-23\",\"2016-05-23\")\n",
    "matrix_train = df_train.as_matrix()\n",
    "matrix_test = df_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arima = ARIMA(p = 2, d = 0, q = 1, seasonal = 1)\n",
    "arima.fit(matrix_train)\n",
    "print str(arima.mae(matrix_test))\n",
    "arima = ARIMA(p = 2, d = 0, q = 2, seasonal = 1)\n",
    "arima.fit(matrix_train)\n",
    "print str(arima.mae(matrix_test))\n",
    "arima = ARIMA(p = 2, d = 0, q = 3, seasonal = 1)\n",
    "arima.fit(matrix_train)\n",
    "print str(arima.mae(matrix_test))\n",
    "print \"\\n\"\n",
    "\n",
    "arima = ARIMA(p = 2, d = 0, q = 3, seasonal = 20)\n",
    "arima.fit(matrix_train)\n",
    "print str(arima.mae(matrix_test))\n",
    "\n",
    "merged_prediction, merged_actual = \\\n",
    "    arima_forcast(matrix_train, matrix_test, 24, 2, 0, 1, 1)\n",
    "plt.plot(merged_actual, label='actual')\n",
    "plt.plot(merged_prediction, label='prediction', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print \"Merged MAE: \" + str(mae(merged_actual, merged_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ercot = ercot_data_interface(password='Is79t5Is79t5')\n",
    "sources_sinks = ercot.get_sources_sinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(47226, 1)\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nn = ercot.get_nearest_CRR_neighbors(sources_sinks[1])\n",
    "    prices = []\n",
    "    for n in nn:\n",
    "        q = ercot.query_prices(n, '2011-01-01', '2016-5-23').as_matrix()\n",
    "        prices.append(q)\n",
    "        \n",
    "    print len(prices)\n",
    "    print q.shape\n",
    "    \n",
    "    \n",
    "except Exception as error:\n",
    "    print(repr(error))\n",
    "    \n",
    "print type(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fiver = []\n",
    "for ss in sources_sinks:\n",
    "    try:\n",
    "        nn = ercot.get_nearest_CRR_neighbors(ss)\n",
    "        if len(nn) >= 5:\n",
    "            fiver.append(nn)\n",
    "\n",
    "    except Exception as error:\n",
    "        k = 0\n",
    "        # print(repr(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isclose(a, b, rel_tol=1e-09, abs_tol=0.0):\n",
    "    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n",
    "\n",
    "def find_zeroes(list):\n",
    "    x = np.asarray(list).flatten()\n",
    "    count = 0\n",
    "    for e in x:\n",
    "        if isclose(e, 0): count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zeroes = []\n",
    "# index = 0\n",
    "# open('neighbor_zeroes.csv', 'w').close()\n",
    "# for g in fiver:\n",
    "#     group = []\n",
    "#     for n in g:\n",
    "#         q = ercot.query_prices(n,'2011-01-01', '2016-5-23').as_matrix()\n",
    "#         group.append(find_zeroes(q))\n",
    "#     zeroes.append(group)\n",
    "#     temp = [fiver[index][0], zeroes[index]]\n",
    "#     with open('neighbor_zeroes.csv', 'ab') as resultFile:\n",
    "#         wr = csv.writer(resultFile, dialect='excel')\n",
    "#         wr.writerow(temp)\n",
    "#     print str(index) + ' ' + str(temp)\n",
    "#     index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "raw = []\n",
    "with open('neighbor_zeroes.csv', 'rb') as csvFile:\n",
    "    reader = csv.reader(csvFile, dialect='excel')\n",
    "    for row in reader:\n",
    "        raw.append(row)\n",
    "\n",
    "table = []\n",
    "for row in raw:\n",
    "    table.append([row[0], ast.literal_eval(row[1])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible = []\n",
    "for group in table:\n",
    "    over = False\n",
    "    for i in group[1]:\n",
    "        if i > 5000:\n",
    "            over = True\n",
    "    if not over:\n",
    "        row = [group[0], group[1]]\n",
    "        possible.append(row)\n",
    "        \n",
    "count = 0\n",
    "for i in possible:\n",
    "    print i\n",
    "    count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAL_CALSTG1', 'CALALLENSUB9', 'CALALLENSUBT', 'CALALLENSUBY', 'CALALLENSUBZ', 'CALAVERSJKS2', 'CALAVERS_1', 'CALAVERS_2', 'CALAVERS_3', 'CALAVERS_A', 'CALAVERS_B', 'CALAVERS_C', 'CALAVERS_D', 'CALAVER_0005', 'CALAVER_0007', 'CALAVER_0009', 'CALAVER_0012', 'CALAVER_0014', 'CALAVER_0023', 'CALAVER_0025', 'CALAVER_0065', 'CALAVER_0105', 'CALAVER_1280', 'CALAVER_1281', 'CALAVER_1282', 'CALAVER_1285', 'CALAVER_1286', 'CALAVER_1383', 'CALAVER_JKS1', 'CALAVER_JKS2', 'CALAVER_JTD1', 'CALAVER_JTD2', 'CALAVER_OWS1', 'CALAVER_OWS2', 'CALENG1G_1G', 'CALENG2G_2G', 'CALENG3G_3G', 'CALFREST_1G', 'CALFREST_2G', 'CALFREST_3G', 'CALFREST_4G', 'CALFREST_5G', 'CALFREST_6GZ', 'CALI_CRK_L_A', 'CALLAHAN_E_1', 'CALLAHAN_V_A', 'CALLAHA_0016', 'CALLAHA_0018', 'CALLAHA_WND1', 'CALMONT1_8', 'CALMONT1_8N', 'CALMONT1_8O', 'CALMONT1_8P', 'CALMONT1_8Q', 'CALMONT1_9', 'CALMONT1_9A', 'CALUMT1_8Y', 'CALUMT1_8Z', 'CALVERTMIN', 'CALVERT_K', 'CALVERT_KQ', 'CAL_1', 'CAL_2', 'CAL_8065', 'CAL_CALGT1', 'CAL_CALSTG1', 'CAL_GT1_8045', 'CAL_L_A', 'CAL_L_B', 'CAL_ST1_8000', 'CAL_V_A', 'CAL_V_B', 'CAL_WWKS_L_A', 'CAL_WWKS_L_B', 'CAL_WWKS_L_C', 'EB_CALA_0008', 'EB_CALA_0010', 'EB_CALA_0120', 'EB_CALA_0145', 'EB_CALA_2050', 'GENCCALLAH', 'L_MCCALA8_1Y', 'L_MCCALA8_1Z', 'L_MCCALA8_2Y', 'NCALLIHAMSBW', 'NCALTEMP', 'N_MCALLN_L_A', 'N_MCALLN_L_B', 'N_MCALLN_L_C', 'N_MCALLN_V_A', 'N_MCALLN_V_B', 'R_CALAVERS_1', 'R_CALAVERS_2', 'SCALLIHAMSBY', 'S_MCALLN_L_A', 'S_MCALLN_L_B', 'S_MCALLN_V_A', 'S_MCALLN_V_B', 'TNCALIFRNA0X', 'UNOCALDH_8', 'W_MCALLN_K', 'W_MCALLN_L_A']\n"
     ]
    }
   ],
   "source": [
    "ercot = ercot_data_interface(password='Is79t5Is79t5')\n",
    "\n",
    "center = table[len(table) - 12][0]\n",
    "nn = ercot.get_nearest_CRR_neighbors('CAL_CALSTG1')\n",
    "print nn\n",
    "q = ercot.query_prices(nn,'2011-01-01', '2016-5-23')\n",
    "\n",
    "index_to_delete = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FO_FORMOSG9def truncate(f, n):\n",
    "    '''Truncates/pads a float f to n decimal places without rounding'''\n",
    "    s = '{}'.format(f)\n",
    "    if 'e' in s or 'E' in s:\n",
    "        return '{0:.{1}f}'.format(f, n)\n",
    "    i, p, d = s.partition('.')\n",
    "    return '.'.join([i, (d+'0'*n)[:n]])\n",
    "\n",
    "means = []\n",
    "count = 0\n",
    "for row in possible:\n",
    "    center = row[0]\n",
    "    nn = ercot.get_nearest_CRR_neighbors(center)\n",
    "#     q = ercot.query_prices(nn[:len(nn)/2],'2011-01-01', '2016-5-23').as_matrix()\n",
    "    q = ercot.query_prices(nn,'2011-01-01', '2016-5-23').as_matrix()\n",
    "    q = q.T\n",
    "    fstat, pvalue = stats.f_oneway(q[0], q[1], q[2], q[3], q[4])\n",
    "    \n",
    "    m = [center, pvalue]\n",
    "    num = []\n",
    "    for row in q:\n",
    "        num.append(np.mean(row))\n",
    "    \n",
    "    num[:] = [x - num[0] for x in num]\n",
    "    m.append(sum(np.abs(num)))\n",
    "    if pvalue < 0.5:\n",
    "        means.append(m)\n",
    "        print m\n",
    "#     if count == 5:\n",
    "#         break\n",
    "#     count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "center = ['TRENT_TRENT']\n",
    "neighbors = ercot.get_nearest_CRR_neighbors('TRENT_TRENT')\n",
    "for row in table:\n",
    "    nn = ercot.get_nearest_CRR_neighbors(row[0])\n",
    "    found = False\n",
    "    for n in nn:\n",
    "        if found:\n",
    "            break\n",
    "            \n",
    "        for neigh in neighbors:\n",
    "            if n == neigh:\n",
    "                found = True\n",
    "                break\n",
    "    \n",
    "    if not found:\n",
    "        for x in nn:\n",
    "            neighbors.append(x)\n",
    "        center.append(row[0])\n",
    "\n",
    "print len(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in center:\n",
    "#     print c\n",
    "# i = 0\n",
    "# for t in table:\n",
    "#     if t[0] == center[i]:\n",
    "#         print t[0] + \" ----- \" + center[i]\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         print t[0]\n",
    "open('removed_duplicates.csv', 'w').close()\n",
    "for cent in center:\n",
    "    temp = [cent, len(ercot.get_nearest_CRR_neighbors(cent))]\n",
    "    with open('removed_duplicates.csv', 'ab') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        wr.writerow(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
